# Google ADK Plugin System - Comprehensive Explanation

The Plugin system in Google ADK provides a powerful extension mechanism that allows developers to intercept and modify the execution flow at critical points in the agent lifecycle. Unlike agent-specific callbacks, plugins apply globally to all agents within a Runner.

## Core Concepts

### 1. BasePlugin Class
All plugins must extend the `BasePlugin` abstract class, which defines numerous callback methods that can be implemented:

```python
from google.adk.plugins.base_plugin import BasePlugin

class MyCustomPlugin(BasePlugin):
    def __init__(self):
        super().__init__(name="my_custom_plugin")
    
    # Implement desired callback methods
    async def before_run_callback(self, *, invocation_context):
        # Custom logic here
        pass
```

### 2. PluginManager
The `PluginManager` orchestrates plugin execution:
- Maintains a list of registered plugins
- Ensures plugins are called in registration order
- Implements "early exit" behavior for non-None returns
- Handles error propagation

### 3. Callback Points
Plugins can hook into 12 different execution points:

## Plugin Callback Points

### 1. User Message Handling
```python
async def on_user_message_callback(
    self, *, invocation_context, user_message
) -> Optional[types.Content]:
```
- Triggered when a user message is received
- Can modify or replace the user message
- Good for preprocessing or validation

### 2. Execution Lifecycle
```python
async def before_run_callback(self, *, invocation_context) -> Optional[types.Content]:
async def after_run_callback(self, *, invocation_context) -> Optional[None]:
```
- `before_run_callback`: First callback in lifecycle, ideal for setup
- `after_run_callback`: Last callback in lifecycle, ideal for cleanup

### 3. Event Processing
```python
async def on_event_callback(self, *, invocation_context, event) -> Optional[Event]:
```
- Called after each event is generated by the runner
- Can modify or replace events before they're returned to the caller

### 4. Agent Execution
```python
async def before_agent_callback(self, *, agent, callback_context) -> Optional[types.Content]:
async def after_agent_callback(self, *, agent, callback_context) -> Optional[types.Content]:
```
- Hook into agent execution before and after processing
- Can short-circuit agent execution by returning a value

### 5. Model Interactions
```python
async def before_model_callback(self, *, callback_context, llm_request) -> Optional[LlmResponse]:
async def after_model_callback(self, *, callback_context, llm_response) -> Optional[LlmResponse]:
```
- Intercept LLM requests before sending to model
- Process LLM responses after receiving from model
- Ideal for caching, logging, or modifying requests/responses

### 6. Tool Execution
```python
async def before_tool_callback(self, *, tool, tool_args, tool_context) -> Optional[dict]:
async def after_tool_callback(self, *, tool, tool_args, tool_context, result) -> Optional[dict]:
```
- Hook into tool execution before and after calling
- Can modify arguments or results
- Useful for logging, validation, or mocking

### 7. Error Handling
```python
async def on_model_error_callback(self, *, callback_context, llm_request, error) -> Optional[LlmResponse]:
async def on_tool_error_callback(self, *, tool, tool_args, tool_context, error) -> Optional[dict]:
```
- Handle errors gracefully
- Provide fallback responses or recovery mechanisms

## Early Exit Behavior

One of the key features of the Plugin system is "early exit":

```python
async def before_run_callback(self, *, invocation_context):
    # If this returns a non-None value, execution stops
    # and this value is returned immediately
    if should_short_circuit():
        return types.Content(parts=[types.Part(text="Short-circuited response")])
    
    # Return None to continue normal execution
    return None
```

When a plugin callback returns a non-None value:
1. Execution of subsequent plugins for that callback stops
2. The returned value is used instead of continuing with normal processing
3. This allows plugins to completely bypass normal execution flow

## Plugin Registration

Plugins are registered when creating a Runner:

```python
runner = Runner(
    app_name="my_app",
    agent=my_agent,
    session_service=my_session_service,
    plugins=[
        LoggingPlugin(),
        MonitoringPlugin(),
        CachingPlugin(),
        ValidationPlugin()
    ]
)
```

## Built-in Plugins

### LoggingPlugin
A comprehensive logging plugin that demonstrates all callback points:

```python
from google.adk.plugins.logging_plugin import LoggingPlugin

# Simply add it to your runner
runner = Runner(
    # ... other parameters
    plugins=[LoggingPlugin()]
)
```

Features:
- Logs user messages with session context
- Tracks agent execution flow
- Logs LLM requests and responses with token usage
- Logs tool calls with arguments and results
- Logs events and final responses
- Handles errors during model and tool execution

## Creating Custom Plugins

### Basic Plugin Template
```python
from google.adk.plugins.base_plugin import BasePlugin
from google.genai import types

class CustomPlugin(BasePlugin):
    def __init__(self, name="custom_plugin", config=None):
        super().__init__(name=name)
        self.config = config or {}
    
    async def before_run_callback(self, *, invocation_context):
        # Custom logic before run starts
        print(f"[{self.name}] Starting execution for session {invocation_context.session.id}")
        return None  # Continue normal execution
```

### Advanced Plugin Example - Caching
```python
class CachingPlugin(BasePlugin):
    def __init__(self, cache_backend=None):
        super().__init__(name="caching_plugin")
        self.cache = cache_backend or {}
    
    async def before_model_callback(self, *, callback_context, llm_request):
        # Create cache key from request
        cache_key = self._generate_cache_key(llm_request)
        
        # Check if we have a cached response
        if cache_key in self.cache:
            cached_response = self.cache[cache_key]
            print(f"[{self.name}] Cache hit for key {cache_key}")
            return cached_response  # Early exit with cached response
        
        print(f"[{self.name}] Cache miss for key {cache_key}")
        return None  # Continue normal execution
    
    async def after_model_callback(self, *, callback_context, llm_response):
        # Store successful responses in cache
        if llm_response.error_code is None:
            # Generate cache key and store response
            cache_key = self._generate_cache_key_from_context(callback_context)
            self.cache[cache_key] = llm_response
            print(f"[{self.name}] Cached response for key {cache_key}")
        
        return None
    
    def _generate_cache_key(self, llm_request):
        # Create a deterministic cache key from request parameters
        import hashlib
        content_str = str(llm_request.model_dump())
        return hashlib.md5(content_str.encode()).hexdigest()
```

### Monitoring Plugin Example
```python
import time
from collections import defaultdict

class MonitoringPlugin(BasePlugin):
    def __init__(self):
        super().__init__(name="monitoring_plugin")
        self.metrics = defaultdict(list)
        self.start_times = {}
    
    async def before_run_callback(self, *, invocation_context):
        # Track invocation start time
        invocation_id = invocation_context.invocation_id
        self.start_times[invocation_id] = time.time()
        return None
    
    async def after_run_callback(self, *, invocation_context):
        # Calculate and record invocation duration
        invocation_id = invocation_context.invocation_id
        if invocation_id in self.start_times:
            duration = time.time() - self.start_times[invocation_id]
            self.metrics['invocation_duration'].append(duration)
            del self.start_times[invocation_id]
            print(f"[{self.name}] Invocation completed in {duration:.2f}s")
        return None
    
    async def before_model_callback(self, *, callback_context, llm_request):
        # Track LLM request start time
        request_id = f"{callback_context.invocation_id}_{time.time()}"
        self.start_times[request_id] = time.time()
        return None
    
    async def after_model_callback(self, *, callback_context, llm_response):
        # Record LLM request duration and token usage
        request_id = f"{callback_context.invocation_id}_{time.time()}"
        if request_id in self.start_times:
            duration = time.time() - self.start_times[request_id]
            self.metrics['llm_request_duration'].append(duration)
            
            if llm_response.usage_metadata:
                tokens_in = llm_response.usage_metadata.prompt_token_count
                tokens_out = llm_response.usage_metadata.candidates_token_count
                self.metrics['tokens_in'].append(tokens_in)
                self.metrics['tokens_out'].append(tokens_out)
            
            del self.start_times[request_id]
        return None
```

## Plugin Execution Flow

1. **Registration**: Plugins are registered during Runner initialization
2. **Callback Chain**: For each callback point, plugins are called in registration order
3. **Early Exit**: If any plugin returns a non-None value, execution stops and that value is used
4. **Error Handling**: Plugin errors are caught and wrapped in RuntimeError
5. **Propagation**: Modified values are passed to subsequent plugins in the chain

## Best Practices

### 1. Performance Considerations
```python
class EfficientPlugin(BasePlugin):
    def __init__(self):
        super().__init__(name="efficient_plugin")
        self._cache = {}
    
    async def before_model_callback(self, *, callback_context, llm_request):
        # Only do expensive operations when necessary
        if not self._should_process_request(llm_request):
            return None
            
        # Perform processing
        return await self._process_expensively(llm_request)
```

### 2. Error Handling
```python
class RobustPlugin(BasePlugin):
    async def before_run_callback(self, *, invocation_context):
        try:
            # Potentially risky operation
            await self._risky_operation()
        except Exception as e:
            # Log error but don't crash the entire system
            print(f"[{self.name}] Error in before_run: {e}")
            # Continue normal execution
            return None
```

### 3. Configuration
```python
class ConfigurablePlugin(BasePlugin):
    def __init__(self, name="configurable_plugin", **config):
        super().__init__(name=name)
        self.config = {
            'enabled': config.get('enabled', True),
            'log_level': config.get('log_level', 'INFO'),
            'cache_ttl': config.get('cache_ttl', 300),
            'max_retries': config.get('max_retries', 3)
        }
```

## Integration with Agent Callbacks

Plugin callbacks execute **before** agent callbacks:
1. Plugin callback chain executes
2. If no early exit, agent callback chain executes
3. Values propagate through both chains

This allows plugins to globally modify behavior while agents handle specific logic.

## Use Cases

1. **Monitoring & Analytics**: Track performance, usage, and errors
2. **Caching**: Cache expensive LLM calls or tool executions
3. **Logging & Debugging**: Enhanced visibility into execution flow
4. **Security**: Input validation, output sanitization, rate limiting
5. **Cost Control**: Token usage tracking, budget enforcement
6. **Compliance**: Audit trails, data governance
7. **Feature Toggles**: Enable/disable functionality dynamically
8. **A/B Testing**: Route requests to different models or configurations

The Plugin system provides a flexible, powerful extension mechanism that enables developers to customize and enhance ADK behavior without modifying core framework code.